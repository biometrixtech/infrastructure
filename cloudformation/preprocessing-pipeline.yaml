# A template that creates the processing pipeline for biometric data
#
# Copyright 2017 Melon Software Ltd (UK), all rights reserved
#
AWSTemplateFormatVersion: "2010-09-09"
Description: "Creates a pre-processing pipeline"

Parameters:

    Environment:
        Type: "String"
        Description: "The name of the Environment"

    BatchJobVersionDownloadandchunk:
        Type: "String"
        Description: "The version of the downloadandchunk batch job to run"
    BatchJobVersionSessionprocess2:
        Type: "String"
        Description: "The version of the sessionprocess2 batch job to run"
    BatchJobVersionNoop:
        Type: "String"
        Description: "The version of the `noop` batch job to run"
    BatchJobVersionScoring:
        Type: "String"
        Description: "The version of the `scoring` batch job to run"
    BatchJobVersionDatabaseupload:
        Type: "String"
        Description: "The version of the `databaseupload` batch job to run"

    OverrideSnsSuccessTopic:
        Type: "String"
        Default: ""
        Description: "An SNS topic ARN to publish success notifications to"

Metadata:
    "AWS::CloudFormation::Interface":
        ParameterGroups:
          - Label: { default: "Environment" }
            Parameters:
              - "Environment"
              - "OverrideSnsSuccessTopic"
          - Label: { default: "Batch Jobs" }
            Parameters:
              - "BatchJobVersionDownloadandchunk"
              - "BatchJobVersionSessionprocess2"
              - "BatchJobVersionNoop"
              - "BatchJobVersionScoring"
              - "BatchJobVersionDatabaseupload"

        ParameterLabels:
            Environment: { default: "Environment" }
            BatchJobVersionDownloadandchunk: { default: "downloadandchunk" }
            BatchJobVersionSessionprocess2: { default: "sessionprocess2" }
            BatchJobVersionNoop: { default: "noop" }
            BatchJobVersionScoring: { default: "scoring" }
            BatchJobVersionDatabaseupload: { default: "databaseupload" }
            OverrideSnsSuccessTopic: { default: "Success SNS Topic ARN" }

Conditions:
    CreateSnsTopic: { "Fn::Equals": [ { Ref: "OverrideSnsSuccessTopic" }, "" ] }

Resources:

    ##########################################################################################################
    ##  IAM
    ##########################################################################################################

    LambdaTriggerRole:
        Type: "AWS::IAM::Role"
        Properties:
            AssumeRolePolicyDocument:
                Version: "2012-10-17"
                Statement:
                  - Effect: "Allow"
                    Principal: { Service: [ "lambda.amazonaws.com" ] }
                    Action: "sts:AssumeRole"
            ManagedPolicyArns:
              - "arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole"
            Policies:
              - PolicyName: "default"
                PolicyDocument:
                    Version: "2012-10-17"
                    Statement:
                      - Action:
                          - "logs:CreateLogGroup"
                          - "logs:CreateLogStream"
                          - "logs:PutLogEvents"
                        Effect: "Allow"
                        Resource: "*"

                      - Action:
                          - "s3:DeleteObject"
                          - "s3:GetObject"
                          - "s3:ListBucket"
                          - "s3:PutObject"
                          - "states:StartExecution"
                        Effect: "Allow"
                        Resource: "*"
            RoleName: { "Fn::Sub": "preprocessing-${Environment}-lambda-trigger-${AWS::Region}" }

    LambdaExecutionRole:
        Type: "AWS::IAM::Role"
        Properties:
            AssumeRolePolicyDocument:
                Version: "2012-10-17"
                Statement:
                  - Effect: "Allow"
                    Principal: { Service: [ "lambda.amazonaws.com" ] }
                    Action: "sts:AssumeRole"
            ManagedPolicyArns:
              - "arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole"
            Policies:
              - PolicyName: "default"
                PolicyDocument:
                    Version: "2012-10-17"
                    Statement:
                      - Action:
                          - "logs:CreateLogGroup"
                          - "logs:CreateLogStream"
                          - "logs:PutLogEvents"
                          - "cloudwatch:PutMetricData"
                        Effect: "Allow"
                        Resource: "*"

                      - Action:
                          - "sns:Publish"
                          - "lambda:InvokeFunction"
                          - "s3:ListBucket"
                        Effect: "Allow"
                        Resource: "*"

                      - Action:
                          - "ssm:DescribeParameters"
                          - "ssm:GetParameters"
                          - "kms:Decrypt"
                        Effect: "Allow"
                        Resource:
                          - "*" # TODO this should be segregated by environment somehow
            RoleName: { "Fn::Sub": "preprocessing-${Environment}-lambda-${AWS::Region}" }

    ##########################################################################################################
    ##  LAMBDA
    ##########################################################################################################

    LambdaTrigger:
        Type: "AWS::Lambda::Function"
        Properties:
            Code:
                ZipFile: |
                    import boto3, json, os, time
                    from datetime import datetime

                    def handler(event, context):
                        print(event)
                        s3_bucket = event['Records'][0]['s3']['bucket']['name']
                        s3_key = event['Records'][0]['s3']['object']['key'].split('/')[-1]

                        combiner = '_'
                        if combiner in s3_key:
                            # A multipart upload
                            s3_basepath, part = s3_key.split(combiner)
                            if part == 'complete':
                                # Ready to combine
                                s3_basepath = s3_basepath
                                suffixes = [s.replace(s3_basepath + combiner, '') for s in list_s3_files(s3_bucket, s3_basepath)]
                                audit_packets(s3_bucket, s3_basepath, suffixes)
                                part_count = len(suffixes) - 1

                            else:
                                # Not the final part
                                return

                        else:
                            # A complete file
                            s3_basepath = s3_key
                            combiner = ''
                            part_count = 0

                        execution_name = '{}-{}'.format(s3_key, int(time.time()))
                        sfn_client = boto3.client('stepfunctions')
                        res = sfn_client.start_execution(
                            stateMachineArn=os.environ['STATE_MACHINE_ARN'],
                            name=execution_name,
                            input=json.dumps({
                                "Meta": {
                                    "ExecutionArn": "{}:{}".format(os.environ['STATE_MACHINE_ARN'], execution_name),
                                    "ExecutionName": execution_name,
                                },
                                "SourceEvent": {
                                    "S3Bucket": s3_bucket,
                                    "S3BasePath": s3_basepath + combiner,
                                    "PartCount": part_count,
                                    "SensorDataFilename": s3_basepath
                                }
                            })
                        )


                    s3_client = boto3.client('s3')
                    def list_s3_files(bucket, prefix, marker=''):
                        objects_list = []
                        resp = s3_client.list_objects(Bucket=bucket, Prefix=prefix, Marker=marker)
                        objects_list.extend([x['Key'] for x in resp['Contents'] if x['Key'][-8:] != 'combined'])
                        if resp['IsTruncated']:
                            objects_list.extend(list_s3_files(bucket, prefix, objects_list[-1]))
                        return sorted(objects_list)


                    def rename_s3_file(bucket, old_file, new_file):
                        s3_client.copy_object(Bucket=bucket, CopySource="{}/{}".format(bucket, old_file), Key=new_file)
                        s3_client.delete_object(Bucket=bucket, Key=old_file)


                    def audit_packets(bucket, prefix, packets):
                        expected_packets = ["{:04d}".format(i) for i in range(len(packets) - 1)] + ["complete"]

                        if len(packets) != len(expected_packets):
                            # Definitely wrong
                            raise_packet_errors(
                                max(len(expected_packets) - len(packets), 1),
                                "Missing packets in upload, expected {} packets, got {}".format(len(expected_packets), len(packets))
                            )

                        if packets != expected_packets:
                            extra_packets = set(packets).difference(expected_packets)
                            missing_packets = set(expected_packets).difference(packets)
                            print("extra_packets: {}, missing_packets: {}".format(extra_packets, missing_packets))

                            if len(extra_packets) == 1 and len(missing_packets) == 1 and prefix in extra_packets:
                                # https://app.asana.com/0/419313112177425/449967103403639
                                # Occasionally a packet is uploaded with no suffix, so if the un-suffixed filename
                                # appears in the packet list, and there is one suffix missing, rename it to fill in
                                # the gap
                                rename_s3_file(bucket, prefix, prefix + '_' + min(extra_packets))

                            else:
                                # There's a more subtle mismatch
                                raise_packet_errors(
                                    0,
                                    "Corrupted packets uploaded"
                                )

                    def raise_packet_errors(num_packets, message):
                        cloudwatch_client = boto3.client('cloudwatch')
                        cloudwatch_client.put_metric_data(
                            Namespace='Preprocessing',
                            MetricData=[
                                {
                                    'MetricName': 'MissingPackets',
                                    'Dimensions': [
                                        {'Name': 'Environment', 'Value': os.environ['ENVIRONMENT']},
                                    ],
                                    'Timestamp': datetime.utcnow(),
                                    'Value': num_packets,
                                },
                            ]
                        )
                        raise Exception(message)


            Environment:
                Variables:
                    ENVIRONMENT: { Ref: "Environment" }
                    STATE_MACHINE_ARN: { Ref: "StateMachine" }
            Handler: "index.handler"
            MemorySize: "256"
            Runtime: "python3.6"
            Timeout: "60"
            Role: { "Fn::GetAtt" : [ "LambdaTriggerRole", "Arn" ] }
            FunctionName: { "Fn::Sub": "preprocessing-${Environment}-pipeline-trigger" }
            Tags:
              - { Key: "Name", Value: { "Fn::Sub": "preprocessing-${Environment}-pipeline-trigger" } }
              - { Key: "Management", Value: "managed" }
              - { Key: "Project", Value: "preprocessing" }
              - { Key: "Environment", Value: { Ref: "Environment" } }


    LambdaPrepareDownloadAndChunk:
        Type: "AWS::Lambda::Function"
        Properties:
            Code:
                ZipFile: !Sub |
                    def handler(event, context):
                        return {
                            "Meta": {
                                "ActivityArn": "${StepFunctionsActivity}",
                                "ExecutionArn": event['Meta']['ExecutionArn'],
                                "ExecutionName": event['Meta']['ExecutionName'],
                                "Environment": "${Environment}"
                            },
                            "Branches": [
                                {
                                    "Resource": {
                                        "BatchJobQueue": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-queue/preprocessing-${Environment}-compute",
                                        "BatchJobDefinition": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-definition/preprocessing-batchjob${BatchJobVersionDownloadandchunk}",
                                        "BatchJob": "downloadandchunk",
                                        "Memory": 2048,
                                        "Vcpus": 2,
                                    },
                                    "InputPath": "$"
                                }
                            ],
                            "Input": event['SourceEvent']
                        }
            Handler: "index.handler"
            Runtime: "python3.6"
            Timeout: "30"
            Role: { "Fn::GetAtt" : [ "LambdaExecutionRole", "Arn" ] }
            FunctionName: { "Fn::Sub": "preprocessing-${Environment}-pipeline-prepare-downloadandchunk" }
            Tags:
              - { Key: "Name", Value: { "Fn::Sub": "preprocessing-${Environment}-pipeline-prepare-downloadandchunk" } }
              - { Key: "Management", Value: "managed" }
              - { Key: "Project", Value: "preprocessing" }
              - { Key: "Environment", Value: { Ref: "Environment" } }

    LambdaReadPostgres:
        Type: "AWS::Lambda::Function"
        Properties:
            Code:
                ZipFile: !Sub |
                    import boto3, json, os
                    lambda_client = boto3.client('lambda', region_name='us-west-2')
                    def query(query, parameters):
                        res = json.loads(lambda_client.invoke(
                            FunctionName='arn:aws:lambda:us-west-2:887689817172:function:preprocessing-dev-pipeline-querypostgres',
                            Payload=json.dumps({
                                "Queries": [{"Query": query, "Parameters": parameters}],
                                "Config": {"ENVIRONMENT": os.environ['ENVIRONMENT']}
                            }),
                        )['Payload'].read())
                        result, error = res['Results'][0], res['Errors'][0]
                        if error is not None:
                            raise Exception(error)
                        else:
                            return result[0] if len(result) else {}

                    def handler(event, context):
                        sensor_data_filename = event['SourceEvent']['S3BasePath'].rstrip('_')
                        query_results = query(
                            """SELECT
                              session_events.id AS session_event_id,
                              session_events.happened_at::date AS event_date,
                              session_events.training_session_log_id,
                              session_events.hip_n_transform,
                              session_events.training_group_ids,
                              session_events.user_id,
                              users.weight,
                              training_groups.team_id,
                              session_events.session_type
                            FROM session_events
                            LEFT JOIN users ON session_events.user_id=users.id
                            LEFT JOIN training_groups ON training_groups.id = session_events.training_group_ids[1]
                            WHERE session_events.sensor_data_filename = %s""",
                            [sensor_data_filename]
                        )
                        ret = {
                            "SensorDataFilename": sensor_data_filename,
                            "SessionEventId": query_results.get('session_event_id', None),
                            "EventDate": query_results.get('event_date', None),
                            "TrainingSessionLogId": query_results.get('training_session_log_id', None),
                            "HipNTransform": query_results.get('hip_n_transform', None),
                            "TrainingGroupId": query_results.get('training_group_ids', None),
                            "UserId": query_results.get('user_id', None),
                            "UserMass": query_results.get('weight', None),
                            "TeamId": query_results.get('team_id', None),
                            "SessionType": query_results.get('session_type', None) or 1,
                        }
                        query_result = query(
                            "SELECT * FROM fn_get_sensor_data_filename_hist((%s))",
                            [ret['UserId']]
                        )
                        ret["HistoricalFiles"] = query_result
                        return ret

            Environment:
                Variables:
                    ENVIRONMENT: { Ref: "Environment" }
            Handler: "index.handler"
            Runtime: "python3.6"
            Timeout: "30"
            Role: { "Fn::GetAtt" : [ "LambdaExecutionRole", "Arn" ] }
            FunctionName: { "Fn::Sub": "preprocessing-${Environment}-pipeline-readpostgres" }
            Tags:
              - { Key: "Name", Value: { "Fn::Sub": "preprocessing-${Environment}-pipeline-readpostgres" } }
              - { Key: "Management", Value: "managed" }
              - { Key: "Project", Value: "preprocessing" }
              - { Key: "Environment", Value: { Ref: "Environment" } }

    LambdaPrepareSessionProcess2:
        Type: "AWS::Lambda::Function"
        Properties:
            Code:
                ZipFile: !Sub |
                    def handler(event, context):
                        return {
                            "Meta": {
                                "ActivityArn": "${StepFunctionsActivity}",
                                "ExecutionArn": event['Meta']['ExecutionArn'],
                                "ExecutionName": event['Meta']['ExecutionName'],
                                "Environment": "${Environment}",
                                "NoopJobDefinition": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-definition/preprocessing-batchjob${BatchJobVersionNoop}"
                            },
                            "Branches": [
                                {
                                    "Resource": {
                                        "BatchJobQueue": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-queue/preprocessing-${Environment}-compute",
                                        "BatchJobDefinition": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-definition/preprocessing-batchjob${BatchJobVersionSessionprocess2}",
                                        "BatchJob": "sessionprocess2",
                                        "Memory": 1024,
                                        "Vcpus": 2,
                                    },
                                    "InputPath": "$[{}]".format(i)
                                }
                                for i in range(len(event['DownloadAndChunk']['Output']['Filenames']))
                            ],
                            "Input": [
                                {**event["ReadDatabase"], **{"Filename": f}}
                                for f in event['DownloadAndChunk']['Output']['Filenames']
                            ]
                        }
            Handler: "index.handler"
            Runtime: "python3.6"
            Timeout: "30"
            Role: { "Fn::GetAtt" : [ "LambdaExecutionRole", "Arn" ] }
            FunctionName: { "Fn::Sub": "preprocessing-${Environment}-pipeline-prepare-sessionprocess2" }
            Tags:
              - { Key: "Name", Value: { "Fn::Sub": "preprocessing-${Environment}-pipeline-sessionprocess2" } }
              - { Key: "Management", Value: "managed" }
              - { Key: "Project", Value: "preprocessing" }
              - { Key: "Environment", Value: { Ref: "Environment" } }

    LambdaPrepareScoring:
        Type: "AWS::Lambda::Function"
        Properties:
            Code:
                ZipFile: !Sub |
                    def handler(event, context):
                        return {
                            "Meta": {
                                "ActivityArn": "${StepFunctionsActivity}",
                                "ExecutionArn": event['Meta']['ExecutionArn'],
                                "ExecutionName": event['Meta']['ExecutionName'],
                                "Environment": "${Environment}",
                                "NoopJobDefinition": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-definition/preprocessing-batchjob${BatchJobVersionNoop}"
                            },
                            "Branches": [
                                {
                                    "Resource": {
                                        "BatchJobQueue": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-queue/preprocessing-${Environment}-compute",
                                        "BatchJobDefinition": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-definition/preprocessing-batchjob${BatchJobVersionScoring}",
                                        "BatchJob": "scoring",
                                        "Memory": 4096,
                                        "Vcpus": 4,
                                    },
                                    "InputPath": "$"
                                }
                            ],
                            "Input": {
                                **event["ReadDatabase"],
                                **{"Filenames": event['DownloadAndChunk']['Output']['Filenames']}
                            }
                        }
            Handler: "index.handler"
            Runtime: "python3.6"
            Timeout: "30"
            Role: { "Fn::GetAtt" : [ "LambdaExecutionRole", "Arn" ] }
            FunctionName: { "Fn::Sub": "preprocessing-${Environment}-pipeline-prepare-scoring" }
            Tags:
              - { Key: "Name", Value: { "Fn::Sub": "preprocessing-${Environment}-pipeline-scoring" } }
              - { Key: "Management", Value: "managed" }
              - { Key: "Project", Value: "preprocessing" }
              - { Key: "Environment", Value: { Ref: "Environment" } }

    LambdaWritePostgres:
        Type: "AWS::Lambda::Function"
        Properties:
            Code:
                ZipFile: !Sub |
                    import boto3, json, os
                    def handler(event, context):
                        lambda_client = boto3.client('lambda', region_name='us-west-2')
                        res = json.loads(lambda_client.invoke(
                            FunctionName='arn:aws:lambda:us-west-2:887689817172:function:preprocessing-dev-pipeline-querypostgres',
                            Payload=json.dumps({
                                "Queries": [{
                                    "Query": "UPDATE session_events SET session_success=True, updated_at = now() WHERE id = %s",
                                    "Parameters": [event['ReadDatabase']['SessionEventId']]
                                }],
                                "Config": {"ENVIRONMENT": os.environ['ENVIRONMENT']}
                            }),
                        )['Payload'].read())
                        if res['Errors'][0] is not None:
                            raise Exception(res['Errors'][0])
                        return {}

            Environment:
                Variables:
                    ENVIRONMENT: { Ref: "Environment" }
            Handler: "index.handler"
            Runtime: "python3.6"
            Timeout: "30"
            Role: { "Fn::GetAtt" : [ "LambdaExecutionRole", "Arn" ] }
            FunctionName: { "Fn::Sub": "preprocessing-${Environment}-pipeline-writepostgres" }
            Tags:
              - { Key: "Name", Value: { "Fn::Sub": "preprocessing-${Environment}-pipeline-writepostgres" } }
              - { Key: "Management", Value: "managed" }
              - { Key: "Project", Value: "preprocessing" }
              - { Key: "Environment", Value: { Ref: "Environment" } }

    LambdaPrepareWriteMongo:
        Type: "AWS::Lambda::Function"
        Properties:
            Code:
                ZipFile: !Sub |
                    def handler(event, context):
                        return {
                            "Meta": {
                                "ActivityArn": "${StepFunctionsActivity}",
                                "ExecutionArn": event['Meta']['ExecutionArn'],
                                "ExecutionName": event['Meta']['ExecutionName'],
                                "Environment": "${Environment}",
                                "NoopJobDefinition": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-definition/preprocessing-batchjob${BatchJobVersionNoop}"
                            },
                            "Branches": [
                                {
                                    "Resource": {
                                        "BatchJobQueue": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-queue/preprocessing-${Environment}-compute",
                                        "BatchJobDefinition": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-definition/preprocessing-batchjob${BatchJobVersionDatabaseupload}",
                                        "BatchJob": "writemongo",
                                        "Memory": 2048,
                                        "Vcpus": 2,
                                    },
                                    "InputPath": "$[{}]".format(i)
                                }
                                for i in range(len(event['Scoring']['Output']['Filenames']))
                            ],
                            "Input": [
                                {**event["ReadDatabase"], **{"Filename": f}}
                                for f in event['Scoring']['Output']['Filenames']
                            ]
                        }
            Handler: "index.handler"
            Runtime: "python3.6"
            Timeout: "30"
            Role: { "Fn::GetAtt" : [ "LambdaExecutionRole", "Arn" ] }
            FunctionName: { "Fn::Sub": "preprocessing-${Environment}-pipeline-prepare-writemongo" }
            Tags:
              - { Key: "Name", Value: { "Fn::Sub": "preprocessing-${Environment}-pipeline-writemongo" } }
              - { Key: "Management", Value: "managed" }
              - { Key: "Project", Value: "preprocessing" }
              - { Key: "Environment", Value: { Ref: "Environment" } }

    LambdaPrepareAggregateSession:
        Type: "AWS::Lambda::Function"
        Properties:
            Code:
                ZipFile: !Sub |
                    def handler(event, context):
                        return {
                            "Meta": {
                                "ActivityArn": "${StepFunctionsActivity}",
                                "ExecutionArn": event['Meta']['ExecutionArn'],
                                "ExecutionName": event['Meta']['ExecutionName'],
                                "Environment": "${Environment}",
                                "NoopJobDefinition": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-definition/preprocessing-batchjob${BatchJobVersionNoop}"
                            },
                            "Branches": [
                                {
                                    "Resource": {
                                        "BatchJobQueue": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-queue/preprocessing-${Environment}-compute",
                                        "BatchJobDefinition": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-definition/preprocessing-batchjob${BatchJobVersionScoring}",
                                        "BatchJob": "aggregatesession",
                                        "Memory": 4096,
                                        "Vcpus": 4,
                                    },
                                    "InputPath": "$"
                                }
                            ],
                            "Input": event["ReadDatabase"]
                        }
            Handler: "index.handler"
            Runtime: "python3.6"
            Timeout: "30"
            Role: { "Fn::GetAtt" : [ "LambdaExecutionRole", "Arn" ] }
            FunctionName: { "Fn::Sub": "preprocessing-${Environment}-pipeline-prepare-aggregatesession" }
            Tags:
              - { Key: "Name", Value: { "Fn::Sub": "preprocessing-${Environment}-pipeline-prepare-aggregatesession" } }
              - { Key: "Management", Value: "managed" }
              - { Key: "Project", Value: "preprocessing" }
              - { Key: "Environment", Value: { Ref: "Environment" } }

    LambdaPrepareAggregateTwomin:
        Type: "AWS::Lambda::Function"
        Properties:
            Code:
                ZipFile: !Sub |
                    def handler(event, context):
                        return {
                            "Meta": {
                                "ActivityArn": "${StepFunctionsActivity}",
                                "ExecutionArn": event['Meta']['ExecutionArn'],
                                "ExecutionName": event['Meta']['ExecutionName'],
                                "Environment": "${Environment}",
                                "NoopJobDefinition": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-definition/preprocessing-batchjob${BatchJobVersionNoop}"
                            },
                            "Branches": [
                                {
                                    "Resource": {
                                        "BatchJobQueue": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-queue/preprocessing-${Environment}-compute",
                                        "BatchJobDefinition": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-definition/preprocessing-batchjob${BatchJobVersionDatabaseupload}",
                                        "BatchJob": "aggregatetwomin",
                                        "Memory": 2048,
                                        "Vcpus": 2,
                                    },
                                    "InputPath": "$[{}]".format(i)
                                }
                                for i in range(len(event['Scoring']['Output']['Filenames']))
                            ],
                            "Input": [
                                {**event["ReadDatabase"], **{"Filename": f}}
                                for f in event['Scoring']['Output']['Filenames']
                            ]
                        }
            Handler: "index.handler"
            Runtime: "python3.6"
            Timeout: "30"
            Role: { "Fn::GetAtt" : [ "LambdaExecutionRole", "Arn" ] }
            FunctionName: { "Fn::Sub": "preprocessing-${Environment}-pipeline-prepare-aggregatetwomin" }
            Tags:
              - { Key: "Name", Value: { "Fn::Sub": "preprocessing-${Environment}-pipeline-prepare-aggregatetwomin" } }
              - { Key: "Management", Value: "managed" }
              - { Key: "Project", Value: "preprocessing" }
              - { Key: "Environment", Value: { Ref: "Environment" } }

    LambdaPrepareAggregateDateUser:
        Type: "AWS::Lambda::Function"
        Properties:
            Code:
                ZipFile: !Sub |
                    def handler(event, context):
                        return {
                            "Meta": {
                                "ActivityArn": "${StepFunctionsActivity}",
                                "ExecutionArn": event['Meta']['ExecutionArn'],
                                "ExecutionName": event['Meta']['ExecutionName'],
                                "Environment": "${Environment}",
                                "NoopJobDefinition": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-definition/preprocessing-batchjob${BatchJobVersionNoop}"
                            },
                            "Branches": [
                                {
                                    "Resource": {
                                        "BatchJobQueue": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-queue/preprocessing-${Environment}-compute",
                                        "BatchJobDefinition": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-definition/preprocessing-batchjob${BatchJobVersionScoring}",
                                        "BatchJob": "aggregatedateuser",
                                        "Memory": 4096,
                                        "Vcpus": 4,
                                    },
                                    "InputPath": "$"
                                }
                            ],
                            "Input": event["ReadDatabase"]
                        }
            Handler: "index.handler"
            Runtime: "python3.6"
            Timeout: "30"
            Role: { "Fn::GetAtt" : [ "LambdaExecutionRole", "Arn" ] }
            FunctionName: { "Fn::Sub": "preprocessing-${Environment}-pipeline-prepare-aggregatedateuser" }
            Tags:
              - { Key: "Name", Value: { "Fn::Sub": "preprocessing-${Environment}-pipeline-prepare-aggregatedateuser" } }
              - { Key: "Management", Value: "managed" }
              - { Key: "Project", Value: "preprocessing" }
              - { Key: "Environment", Value: { Ref: "Environment" } }

    LambdaPrepareAggregateTeam:
        Type: "AWS::Lambda::Function"
        Properties:
            Code:
                ZipFile: !Sub |
                    def handler(event, context):
                        return {
                            "Meta": {
                                "ActivityArn": "${StepFunctionsActivity}",
                                "ExecutionArn": event['Meta']['ExecutionArn'],
                                "ExecutionName": event['Meta']['ExecutionName'],
                                "Environment": "${Environment}",
                                "NoopJobDefinition": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-definition/preprocessing-batchjob${BatchJobVersionNoop}"
                            },
                            "Branches": [
                                {
                                    "Resource": {
                                        "BatchJobQueue": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-queue/preprocessing-${Environment}-compute",
                                        "BatchJobDefinition": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-definition/preprocessing-batchjob${BatchJobVersionScoring}",
                                        "BatchJob": "aggregateteam",
                                        "Memory": 512,
                                        "Vcpus": 1,
                                    },
                                    "InputPath": "$"
                                }
                            ],
                            "Input": event["ReadDatabase"],
                            "LinearityGroup": event["ReadDatabase"]["TeamId"]
                        }
            Handler: "index.handler"
            Runtime: "python3.6"
            Timeout: "30"
            Role: { "Fn::GetAtt" : [ "LambdaExecutionRole", "Arn" ] }
            FunctionName: { "Fn::Sub": "preprocessing-${Environment}-pipeline-prepare-aggregateteam" }
            Tags:
              - { Key: "Name", Value: { "Fn::Sub": "preprocessing-${Environment}-pipeline-prepare-aggregateteam" } }
              - { Key: "Management", Value: "managed" }
              - { Key: "Project", Value: "preprocessing" }
              - { Key: "Environment", Value: { Ref: "Environment" } }

    LambdaNotifyCSharp:
        Type: "AWS::Lambda::Function"
        Properties:
            Code:
                ZipFile: !Sub |
                    def handler(event, context):
                        import boto3, os
                        sns_client = boto3.client('sns')
                        response = sns_client.publish(
                            TopicArn=os.environ['SNS_TOPIC'],
                            Message=event['ReadDatabase']['SessionEventId'],
                        )
            Environment:
                Variables:
                    SNS_TOPIC: { "Fn::If": [ "CreateSnsTopic", { Ref: "SnsSuccessTopic" }, { Ref: "OverrideSnsSuccessTopic" } ] }
            Handler: "index.handler"
            Runtime: "python3.6"
            Timeout: "30"
            Role: { "Fn::GetAtt" : [ "LambdaExecutionRole", "Arn" ] }
            FunctionName: { "Fn::Sub": "preprocessing-${Environment}-pipeline-notifycsharp" }
            Tags:
              - { Key: "Name", Value: { "Fn::Sub": "preprocessing-${Environment}-pipeline-notifycsharp" } }
              - { Key: "Management", Value: "managed" }
              - { Key: "Project", Value: "preprocessing" }
              - { Key: "Environment", Value: { Ref: "Environment" } }

    LambdaNotifyFailure:
        Type: "AWS::Lambda::Function"
        Properties:
            Code:
                ZipFile: !Sub |
                    def handler(event, context):
                        import boto3, os
                        sns_client = boto3.client('sns')
                        response = sns_client.publish(
                            TopicArn=os.environ['SNS_TOPIC'],
                            Message=event['ReadDatabase']['SessionEventId'],
                        )
            Environment:
                Variables:
                    SNS_TOPIC: { Ref: "SnsFailureTopic" }
            Handler: "index.handler"
            Runtime: "python3.6"
            Timeout: "30"
            Role: { "Fn::GetAtt" : [ "LambdaExecutionRole", "Arn" ] }
            FunctionName: { "Fn::Sub": "preprocessing-${Environment}-pipeline-notifyfailure" }
            Tags:
              - { Key: "Name", Value: { "Fn::Sub": "preprocessing-${Environment}-pipeline-notifyfailure" } }
              - { Key: "Management", Value: "managed" }
              - { Key: "Project", Value: "preprocessing" }
              - { Key: "Environment", Value: { Ref: "Environment" } }

    LambdaPrepareCleanup:
        Type: "AWS::Lambda::Function"
        Properties:
            Code:
                ZipFile: !Sub |
                    def handler(event, context):
                        return {
                            "Meta": {
                                "ActivityArn": "${StepFunctionsActivity}",
                                "ExecutionArn": event['Meta']['ExecutionArn'],
                                "ExecutionName": event['Meta']['ExecutionName'],
                                "Environment": "${Environment}",
                                "NoopJobDefinition": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-definition/preprocessing-batchjob${BatchJobVersionNoop}"
                            },
                            "Branches": [
                                {
                                    "Resource": {
                                        "BatchJobQueue": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-queue/preprocessing-${Environment}-compute",
                                        "BatchJobDefinition": "arn:aws:batch:${AWS::Region}:${AWS::AccountId}:job-definition/preprocessing-batchjob${BatchJobVersionScoring}",
                                        "BatchJob": "cleanup",
                                        "Memory": 256,
                                        "Vcpus": 2,
                                    },
                                    "InputPath": "$"
                                }
                            ],
                            "Input": event["ReadDatabase"]
                        }
            Handler: "index.handler"
            Runtime: "python3.6"
            Timeout: "30"
            Role: { "Fn::GetAtt" : [ "LambdaExecutionRole", "Arn" ] }
            FunctionName: { "Fn::Sub": "preprocessing-${Environment}-pipeline-prepare-cleanup" }
            Tags:
              - { Key: "Name", Value: { "Fn::Sub": "preprocessing-${Environment}-pipeline-cleanup" } }
              - { Key: "Management", Value: "managed" }
              - { Key: "Project", Value: "preprocessing" }
              - { Key: "Environment", Value: { Ref: "Environment" } }

    ##########################################################################################################
    ##  SQS
    ##########################################################################################################

    LinearityEnforcingQueue:
        Type: "AWS::SQS::Queue"
        Properties:
            ContentBasedDeduplication: false
            FifoQueue: true
            MessageRetentionPeriod: 86400
            QueueName: { "Fn::Sub": "preprocessing-${Environment}-linearity.fifo" }
            ReceiveMessageWaitTimeSeconds: 20
            VisibilityTimeout: 3600

    ##########################################################################################################
    ##  SNS
    ##########################################################################################################

    SnsSuccessTopic:
        Type: "AWS::SNS::Topic"
        Properties:
            DisplayName: "Biometrix preprocessing success notification"
            TopicName: { "Fn::Sub": "preprocessing-${Environment}-success" }
        Condition: "CreateSnsTopic"

    SnsFailureTopic:
        Type: "AWS::SNS::Topic"
        Properties:
            DisplayName: "Biometrix preprocessing failure notification"
            TopicName: { "Fn::Sub": "preprocessing-${Environment}-failure" }

    ##########################################################################################################
    ##  STEP FUNCTIONS
    ##########################################################################################################

    StepFunctionsActivity:
        Type: "AWS::StepFunctions::Activity"
        Properties:
            Name: { "Fn::Sub": "preprocessing-${Environment}" }

    StateMachine:
        Type: "AWS::StepFunctions::StateMachine"
        Properties:
            DefinitionString: !Sub |
                {
                    "StartAt": "BuildDownloadAndChunkContext",
                    "States": {
                        "BuildDownloadAndChunkContext": {
                            "Type": "Task",
                            "Resource": "${LambdaPrepareDownloadAndChunk.Arn}",
                            "ResultPath": "$.DownloadAndChunk",
                            "Next": "DownloadAndChunk"
                        },
                        "DownloadAndChunk": {
                            "Type": "Task",
                            "InputPath": "$.DownloadAndChunk",
                            "Resource": "${StepFunctionsActivity}",
                            "ResultPath": "$.DownloadAndChunk",
                            "Next": "ReadDatabase"
                        },
                        "ReadDatabase": {
                            "Type": "Task",
                            "Resource": "${LambdaReadPostgres.Arn}",
                            "ResultPath": "$.ReadDatabase",
                            "Next": "BuildSessionProcess2Context"
                        },
                        "BuildSessionProcess2Context": {
                            "Type": "Task",
                            "Resource": "${LambdaPrepareSessionProcess2.Arn}",
                            "ResultPath": "$.SessionProcess2",
                            "Next": "SessionProcess2"
                        },
                        "SessionProcess2": {
                            "Type": "Task",
                            "InputPath": "$.SessionProcess2",
                            "Resource": "${StepFunctionsActivity}",
                            "ResultPath": "$.SessionProcess2",
                            "Next": "BuildScoringContext"
                        },
                        "BuildScoringContext": {
                            "Type": "Task",
                            "Resource": "${LambdaPrepareScoring.Arn}",
                            "ResultPath": "$.Scoring",
                            "Next": "Scoring"
                        },
                        "Scoring": {
                            "Type": "Task",
                            "InputPath": "$.Scoring",
                            "Resource": "${StepFunctionsActivity}",
                            "Catch": [
                                {
                                    "ErrorEquals": [ "NoHistoricalDataException" ],
                                    "Next": "ExceptionNoHistoricalData",
                                    "ResultPath": "$._ExceptionOutput"
                                }
                            ],
                            "ResultPath": "$.Scoring",
                            "Next": "Aggregate1"
                        },
                        "Aggregate1": {
                            "Type": "Parallel",
                            "Branches": [
                                {
                                    "StartAt": "WritePostgres",
                                    "States": {
                                        "WritePostgres": {
                                            "Type": "Task",
                                            "Resource": "${LambdaWritePostgres.Arn}",
                                            "End": true
                                        }
                                    }
                                },
                                {
                                    "StartAt": "BuildWriteMongoContext",
                                    "States": {
                                        "BuildWriteMongoContext": {
                                            "Type": "Task",
                                            "Resource": "${LambdaPrepareWriteMongo.Arn}",
                                            "ResultPath": "$.WriteMongo",
                                            "Next": "WriteMongo"
                                        },
                                        "WriteMongo": {
                                            "Type": "Task",
                                            "InputPath": "$.WriteMongo",
                                            "Resource": "${StepFunctionsActivity}",
                                            "ResultPath": "$.WriteMongo",
                                            "Next": "NotifyCSharp"
                                        },
                                        "NotifyCSharp": {
                                            "Type": "Task",
                                            "Resource": "${LambdaNotifyCSharp.Arn}",
                                            "End": true
                                        }
                                    }
                                },
                                {
                                    "StartAt": "BuildAggregateSessionContext",
                                    "States": {
                                        "BuildAggregateSessionContext": {
                                            "Type": "Task",
                                            "Resource": "${LambdaPrepareAggregateSession.Arn}",
                                            "ResultPath": "$.AggregateSession",
                                            "Next": "AggregateSession"
                                        },
                                        "AggregateSession": {
                                            "Type": "Task",
                                            "InputPath": "$.AggregateSession",
                                            "Resource": "${StepFunctionsActivity}",
                                            "ResultPath": "$.AggregateSession",
                                            "Next": "BuildAggregateDateContext"
                                        },
                                        "BuildAggregateDateContext": {
                                            "Type": "Task",
                                            "Resource": "${LambdaPrepareAggregateDateUser.Arn}",
                                            "ResultPath": "$.AggregateDate",
                                            "Next": "AggregateDate"
                                        },
                                        "AggregateDate": {
                                            "Type": "Task",
                                            "InputPath": "$.AggregateDate",
                                            "Resource": "${StepFunctionsActivity}",
                                            "ResultPath": "$.AggregateDate",
                                            "End": true
                                        }
                                    }
                                },
                                {
                                    "StartAt": "BuildAggregateTwominContext",
                                    "States": {
                                        "BuildAggregateTwominContext": {
                                            "Type": "Task",
                                            "Resource": "${LambdaPrepareAggregateTwomin.Arn}",
                                            "ResultPath": "$.AggregateTwomin",
                                            "Next": "AggregateTwomin"
                                        },
                                        "AggregateTwomin": {
                                            "Type": "Task",
                                            "InputPath": "$.AggregateTwomin",
                                            "Resource": "${StepFunctionsActivity}",
                                            "ResultPath": "$.AggregateTwomin",
                                            "End": true
                                        }
                                    }
                                }
                            ],
                            "ResultPath": null,
                            "Next": "Aggregate2"
                        },
                        "Aggregate2": {
                            "Type": "Parallel",
                            "Branches": [
                                {
                                    "StartAt": "BuildAggregateTeamContext",
                                    "States": {
                                        "BuildAggregateTeamContext": {
                                            "Type": "Task",
                                            "Resource": "${LambdaPrepareAggregateTeam.Arn}",
                                            "ResultPath": "$.AggregateTeam",
                                            "Next": "AggregateTeam"
                                        },
                                        "AggregateTeam": {
                                            "Type": "Task",
                                            "InputPath": "$.AggregateTeam",
                                            "Resource": "${StepFunctionsActivity}",
                                            "ResultPath": "$.AggregateTeam",
                                            "End": true
                                        }
                                    }
                                }
                            ],
                            "ResultPath": null,
                            "Next": "Post"
                        },
                        "Post": {
                            "Type": "Parallel",
                            "Branches": [
                                {
                                    "StartAt": "BuildCleanupContext",
                                    "States": {
                                        "BuildCleanupContext": {
                                            "Type": "Task",
                                            "Resource": "${LambdaPrepareCleanup.Arn}",
                                            "ResultPath": "$.Cleanup",
                                            "Next": "Cleanup"
                                        },
                                        "Cleanup": {
                                            "Type": "Task",
                                            "InputPath": "$.Cleanup",
                                            "Resource": "${StepFunctionsActivity}",
                                            "ResultPath": "$.Cleanup",
                                            "End": true
                                        }
                                    }
                                }
                            ],
                            "ResultPath": "$.Cleanup",
                            "End": true
                        },
                        "ExceptionNoHistoricalData": {
                            "Type": "Task",
                            "Resource": "${LambdaNotifyFailure.Arn}",
                            "Next": "ExceptionStop"
                        },
                        "ExceptionStop": {
                            "Type": "Fail"
                        }
                    }
                }

            RoleArn: { "Fn::ImportValue" : "StepFunctionsServiceRole" }
Outputs:
    ActivityArn:
        Description: "The ARN of the Batch Activity"
        Value: { Ref: "StepFunctionsActivity" }
    LinearitySqsQueueName:
        Description: "The name of the linearity-enforcing SQS Queue"
        Value: { Ref: "LinearityEnforcingQueue" }
    TriggerLambdaArn:
        Description: "The ARN of the trigger lambda"
        Value: { "Fn::GetAtt": [ "LambdaTrigger", "Arn" ] }
    StateMachineArn:
        Description: "The ARN of the SFN state machine"
        Value: { Ref: "StateMachine" }
